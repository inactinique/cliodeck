import { randomUUID } from 'crypto';
import { PDFExtractor } from './PDFExtractor.js';
import { TextPreprocessor } from './TextPreprocessor.js';
import { DocumentChunker, CHUNKING_CONFIGS, type ChunkingConfig } from '../chunking/DocumentChunker.js';
import { AdaptiveChunker } from '../chunking/AdaptiveChunker.js';
import { SemanticChunker } from '../chunking/SemanticChunker.js';
import { ChunkQualityScorer } from '../chunking/ChunkQualityScorer.js';
import { ChunkDeduplicator } from '../chunking/ChunkDeduplicator.js';
import { EmbeddingCache } from '../chunking/EmbeddingCache.js';
import { VectorStore } from '../vector-store/VectorStore.js';
import { EnhancedVectorStore } from '../vector-store/EnhancedVectorStore.js';
import { OllamaClient } from '../llm/OllamaClient.js';
import { CitationExtractor } from '../analysis/CitationExtractor.js';
import { DocumentSummarizer, type SummarizerConfig } from '../analysis/DocumentSummarizer.js';
import type { PDFDocument, DocumentChunk } from '../../types/pdf-document.js';
import type { RAGConfig } from '../../types/config.js';

/**
 * Extended indexing options including all RAG optimization features
 */
export interface IndexingOptions {
  // Chunking
  chunkingPreset?: 'cpuOptimized' | 'standard' | 'large';
  customChunkingEnabled?: boolean;
  customMaxChunkSize?: number;
  customMinChunkSize?: number;
  customOverlapSize?: number;
  useAdaptiveChunking?: boolean;

  // Quality filtering
  enableQualityFiltering?: boolean;
  minChunkEntropy?: number;
  minUniqueWordRatio?: number;

  // Preprocessing
  enablePreprocessing?: boolean;
  enableOCRCleanup?: boolean;
  enableHeaderFooterRemoval?: boolean;

  // Deduplication
  enableDeduplication?: boolean;
  enableSimilarityDedup?: boolean;
  dedupSimilarityThreshold?: number;

  // Semantic chunking
  useSemanticChunking?: boolean;
  semanticSimilarityThreshold?: number;
  semanticWindowSize?: number;

  // Summarizer
  summarizerConfig?: SummarizerConfig;
}

export interface IndexingProgress {
  stage:
    | 'extracting'
    | 'analyzing'
    | 'citations'
    | 'summarizing'
    | 'chunking'
    | 'embedding'
    | 'similarities'
    | 'completed'
    | 'error';
  progress: number; // 0-100
  message: string;
  currentPage?: number;
  totalPages?: number;
  currentChunk?: number;
  totalChunks?: number;
}

export class PDFIndexer {
  private pdfExtractor: PDFExtractor;
  private textPreprocessor: TextPreprocessor;
  private chunker: DocumentChunker | AdaptiveChunker;
  private semanticChunker: SemanticChunker | null = null;
  private qualityScorer: ChunkQualityScorer;
  private deduplicator: ChunkDeduplicator;
  private embeddingCache: EmbeddingCache;
  private vectorStore: VectorStore | EnhancedVectorStore;
  private ollamaClient: OllamaClient;
  private citationExtractor: CitationExtractor;
  private documentSummarizer: DocumentSummarizer | null = null;
  private summarizerConfig: SummarizerConfig;
  private options: IndexingOptions;

  constructor(
    vectorStore: VectorStore | EnhancedVectorStore,
    ollamaClient: OllamaClient,
    chunkingConfig: 'cpuOptimized' | 'standard' | 'large' = 'cpuOptimized',
    summarizerConfig?: SummarizerConfig,
    useAdaptiveChunking: boolean = false,
    ragConfig?: Partial<RAGConfig>
  ) {
    this.pdfExtractor = new PDFExtractor();
    this.textPreprocessor = new TextPreprocessor();
    this.qualityScorer = new ChunkQualityScorer();
    this.deduplicator = new ChunkDeduplicator();
    this.embeddingCache = new EmbeddingCache(500);
    this.vectorStore = vectorStore;
    this.ollamaClient = ollamaClient;
    this.citationExtractor = new CitationExtractor();

    // Build options from ragConfig or use defaults
    this.options = {
      chunkingPreset: chunkingConfig,
      useAdaptiveChunking: ragConfig?.useAdaptiveChunking ?? useAdaptiveChunking,
      customChunkingEnabled: ragConfig?.customChunkingEnabled ?? false,
      customMaxChunkSize: ragConfig?.customMaxChunkSize ?? 500,
      customMinChunkSize: ragConfig?.customMinChunkSize ?? 100,
      customOverlapSize: ragConfig?.customOverlapSize ?? 75,
      enableQualityFiltering: ragConfig?.enableQualityFiltering ?? true,
      minChunkEntropy: ragConfig?.minChunkEntropy ?? 0.3,
      minUniqueWordRatio: ragConfig?.minUniqueWordRatio ?? 0.4,
      enablePreprocessing: ragConfig?.enablePreprocessing ?? true,
      enableOCRCleanup: ragConfig?.enableOCRCleanup ?? true,
      enableHeaderFooterRemoval: ragConfig?.enableHeaderFooterRemoval ?? true,
      enableDeduplication: ragConfig?.enableDeduplication ?? true,
      enableSimilarityDedup: ragConfig?.enableSimilarityDedup ?? false,
      dedupSimilarityThreshold: ragConfig?.dedupSimilarityThreshold ?? 0.85,
      useSemanticChunking: ragConfig?.useSemanticChunking ?? false,
      semanticSimilarityThreshold: ragConfig?.semanticSimilarityThreshold ?? 0.7,
      semanticWindowSize: ragConfig?.semanticWindowSize ?? 3,
      summarizerConfig,
    };

    // Build chunking config
    let chunkingCfg: ChunkingConfig;
    if (this.options.customChunkingEnabled) {
      chunkingCfg = {
        maxChunkSize: this.options.customMaxChunkSize!,
        minChunkSize: this.options.customMinChunkSize!,
        overlapSize: this.options.customOverlapSize!,
      };
      console.log('üìê Using custom chunking config:', chunkingCfg);
    } else {
      chunkingCfg = CHUNKING_CONFIGS[chunkingConfig];
    }

    // Choose chunker based on configuration
    if (this.options.useAdaptiveChunking) {
      console.log('üìê Using AdaptiveChunker (structure-aware)');
      this.chunker = new AdaptiveChunker(chunkingCfg);
    } else {
      console.log('üìê Using DocumentChunker (fixed-size)');
      this.chunker = new DocumentChunker(chunkingCfg);
    }

    // Initialize semantic chunker if enabled
    if (this.options.useSemanticChunking) {
      console.log('üß† Semantic chunking enabled');
      this.semanticChunker = new SemanticChunker(
        (text) => this.ollamaClient.generateEmbedding(text),
        {
          similarityThreshold: this.options.semanticSimilarityThreshold!,
          windowSize: this.options.semanticWindowSize!,
          minChunkSize: chunkingCfg.minChunkSize,
          maxChunkSize: chunkingCfg.maxChunkSize,
        },
        this.embeddingCache
      );
    }

    // Log enabled features
    console.log('üîß [INDEXER] RAG optimization features:');
    console.log(`   - Preprocessing: ${this.options.enablePreprocessing}`);
    console.log(`   - Quality filtering: ${this.options.enableQualityFiltering}`);
    console.log(`   - Deduplication: ${this.options.enableDeduplication}`);
    console.log(`   - Semantic chunking: ${this.options.useSemanticChunking}`);

    // Initialiser DocumentSummarizer si activ√©
    this.summarizerConfig = summarizerConfig || {
      enabled: false,
      method: 'extractive',
      maxLength: 250,
    };

    if (this.summarizerConfig.enabled) {
      this.documentSummarizer = new DocumentSummarizer(this.summarizerConfig, ollamaClient);
    }
  }

  /**
   * Indexe un PDF complet
   * @param filePath Chemin vers le fichier PDF
   * @param bibtexKey Cl√© BibTeX optionnelle pour lier √† la bibliographie
   * @param onProgress Callback pour la progression
   * @param bibliographyMetadata M√©tadonn√©es optionnelles provenant de la bibliographie (prioritaires sur l'extraction PDF)
   */
  async indexPDF(
    filePath: string,
    bibtexKey?: string,
    onProgress?: (progress: IndexingProgress) => void,
    bibliographyMetadata?: { title?: string; author?: string; year?: string }
  ): Promise<PDFDocument> {
    console.log('üîç [INDEXER] Starting PDF indexing...');
    console.log(`   File: ${filePath}`);
    console.log(`   BibtexKey: ${bibtexKey || 'none'}`);

    try {
      // 1. Extraire le texte + m√©tadonn√©es
      console.log('üîç [INDEXER] Step 1: Extracting text and metadata...');
      onProgress?.({
        stage: 'extracting',
        progress: 10,
        message: 'Extraction du texte PDF...',
      });

      const { pages, metadata, title: extractedTitle } = await this.pdfExtractor.extractDocument(filePath);
      console.log(`üîç [INDEXER] Step 1 complete: ${pages.length} pages extracted`);

      // Use bibliography metadata if provided, otherwise fall back to PDF extraction
      const title = bibliographyMetadata?.title || extractedTitle;

      onProgress?.({
        stage: 'extracting',
        progress: 25,
        message: `${pages.length} pages extraites`,
        totalPages: pages.length,
      });

      // 2. Use bibliography metadata for author/year if provided, otherwise extract from PDF
      let author: string | undefined;
      let year: string | undefined;

      if (bibliographyMetadata?.author) {
        author = bibliographyMetadata.author;
        console.log(`   Using bibliography author: ${author}`);
      } else {
        author = await this.pdfExtractor.extractAuthor(filePath);
      }

      if (bibliographyMetadata?.year) {
        year = bibliographyMetadata.year;
        console.log(`   Using bibliography year: ${year}`);
      } else {
        year = await this.pdfExtractor.extractYear(filePath);
      }

      // 3. Extraire le texte complet pour analyse
      const fullText = pages.map((p) => p.text).join('\n\n');

      // 4. D√©tecter la langue du document
      onProgress?.({
        stage: 'analyzing',
        progress: 27,
        message: 'Analyse du document...',
      });

      const language = this.citationExtractor.detectLanguage(fullText);
      console.log(`   Langue d√©tect√©e: ${language}`);

      // 5. Extraction des citations
      onProgress?.({
        stage: 'citations',
        progress: 30,
        message: 'Extraction des citations...',
      });

      const citations = this.citationExtractor.extractCitations(fullText, pages);
      console.log(`   Citations extraites: ${citations.length}`);

      // Statistiques sur les citations
      if (citations.length > 0) {
        const stats = this.citationExtractor.getCitationStatistics(citations);
        console.log(
          `   - ${stats.totalCitations} citations, ${stats.uniqueAuthors} auteurs, ann√©es ${stats.yearRange.min}-${stats.yearRange.max}`
        );
      }

      // 6. G√©n√©ration du r√©sum√© (optionnel)
      let summary: string | undefined;
      let summaryEmbedding: Float32Array | undefined;

      if (this.documentSummarizer) {
        onProgress?.({
          stage: 'summarizing',
          progress: 33,
          message: `G√©n√©ration du r√©sum√© (${this.summarizerConfig.method})...`,
        });

        summary = await this.documentSummarizer.generateSummary(fullText, metadata);

        // G√©n√©rer l'embedding du r√©sum√©
        if (summary) {
          summaryEmbedding = await this.documentSummarizer.generateSummaryEmbedding(summary);
          console.log(`   R√©sum√© g√©n√©r√©: ${summary.split(' ').length} mots`);
        }
      }

      // 7. Cr√©er le document avec donn√©es enrichies
      const documentId = randomUUID();
      const now = new Date();

      const document: PDFDocument = {
        id: documentId,
        fileURL: filePath,
        title,
        author,
        year,
        bibtexKey,
        pageCount: pages.length,
        metadata,
        createdAt: now,
        indexedAt: now,
        lastAccessedAt: now,
        get displayString() {
          if (this.author && this.year) {
            return `${this.author} (${this.year})`;
          }
          return this.title;
        },
      };

      // Ajouter les champs enrichis
      (document as any).language = language;
      (document as any).citationsExtracted = citations;
      (document as any).summary = summary;
      (document as any).summaryEmbedding = summaryEmbedding;

      // 8. Sauvegarder le document
      this.vectorStore.saveDocument(document);

      // 9. Matcher et sauvegarder les citations avec documents existants
      const allDocuments = this.vectorStore.getAllDocuments();
      const citationMatches = this.citationExtractor.matchCitationsWithDocuments(
        citations,
        allDocuments
      );

      // Sauvegarder les citations match√©es en BDD
      for (const citation of citations) {
        const citationId = randomUUID();
        const targetDocId = citationMatches.get(citation.id);

        this.vectorStore.saveCitation({
          id: citationId,
          sourceDocId: documentId,
          targetCitation: citation.text,
          targetDocId,
          context: citation.context,
          pageNumber: citation.pageNumber,
        });
      }

      if (citationMatches.size > 0) {
        console.log(`   Citations match√©es: ${citationMatches.size}/${citations.length}`);
      }

      // 10. Preprocess pages (if enabled)
      let processedPages = pages;
      if (this.options.enablePreprocessing) {
        onProgress?.({
          stage: 'chunking',
          progress: 38,
          message: 'Pr√©traitement du texte...',
        });

        const preprocessResult = this.textPreprocessor.preprocess(pages, {
          enableOCRCleanup: this.options.enableOCRCleanup,
          enableHeaderFooterRemoval: this.options.enableHeaderFooterRemoval,
          enablePageNumberRemoval: true,
        });
        processedPages = preprocessResult.pages;

        console.log(`üßπ [PREPROCESS] Stats:`, {
          headersRemoved: preprocessResult.stats.headersRemoved,
          footersRemoved: preprocessResult.stats.footersRemoved,
          pageNumbersRemoved: preprocessResult.stats.pageNumbersRemoved,
          charactersRemoved: preprocessResult.stats.charactersRemoved,
        });
      }

      // 11. Cr√©er les chunks
      onProgress?.({
        stage: 'chunking',
        progress: 40,
        message: 'D√©coupage du texte en chunks...',
      });

      // Pass document metadata to chunker for context enhancement
      const documentMeta = {
        title: document.title,
        abstract: summary,
      };

      let chunks: DocumentChunk[];

      // Use semantic chunker if enabled, otherwise use regular chunker
      if (this.semanticChunker && this.options.useSemanticChunking) {
        console.log('üß† [SEMANTIC] Using semantic chunking...');
        chunks = await this.semanticChunker.createChunks(processedPages, documentId, documentMeta);
      } else if (this.chunker instanceof AdaptiveChunker) {
        chunks = this.chunker.createChunks(processedPages, documentId, documentMeta);
      } else {
        chunks = this.chunker.createChunks(processedPages, documentId);
      }

      console.log(`üìä Initial chunking: ${chunks.length} chunks created`);

      // 12. Quality filtering (if enabled)
      if (this.options.enableQualityFiltering) {
        const qualityResult = this.qualityScorer.filterByQuality(chunks, {
          minEntropy: this.options.minChunkEntropy,
          minUniqueWordRatio: this.options.minUniqueWordRatio,
        }, false); // Don't log each filtered chunk

        console.log(`üéØ [QUALITY] Filtering: ${qualityResult.stats.passedChunks}/${qualityResult.stats.totalChunks} passed (${(qualityResult.stats.filterRate * 100).toFixed(1)}% filtered)`);
        chunks = qualityResult.passed;
      }

      // 13. Deduplication (if enabled)
      if (this.options.enableDeduplication) {
        const dedupResult = this.deduplicator.deduplicate(chunks, {
          useContentHash: true,
          useSimilarity: this.options.enableSimilarityDedup,
          similarityThreshold: this.options.dedupSimilarityThreshold!,
        });

        if (dedupResult.duplicateCount > 0) {
          console.log(`üîÑ [DEDUP] Removed ${dedupResult.duplicateCount} duplicate chunks`);
        }
        chunks = dedupResult.uniqueChunks;
      }

      const stats = this.chunker.getChunkingStats(chunks);
      console.log(
        `üìä Final: ${stats.totalChunks} chunks, ${stats.averageWordCount} mots/chunk en moyenne`
      );

      onProgress?.({
        stage: 'chunking',
        progress: 45,
        message: `${chunks.length} chunks cr√©√©s`,
        totalChunks: chunks.length,
      });

      // 11. G√©n√©rer les embeddings et sauvegarder
      onProgress?.({
        stage: 'embedding',
        progress: 50,
        message: 'G√©n√©ration des embeddings...',
        totalChunks: chunks.length,
      });

      // Check if we're using EnhancedVectorStore
      const isEnhancedStore = this.vectorStore instanceof EnhancedVectorStore;
      console.log(`üîç [INDEXER] Step 7: Generating embeddings (EnhancedStore: ${isEnhancedStore})...`);

      if (isEnhancedStore) {
        // Batch processing for EnhancedVectorStore
        console.log('üì¶ Using batch indexing for EnhancedVectorStore');
        const chunksWithEmbeddings = [];

        for (let i = 0; i < chunks.length; i++) {
          const chunk = chunks[i];

          // G√©n√©rer l'embedding
          console.log(`üîç [INDEXER] Generating embedding ${i + 1}/${chunks.length}...`);
          const embedding = await this.ollamaClient.generateEmbedding(chunk.content);
          console.log(`üîç [INDEXER] Embedding ${i + 1} generated, dimension: ${embedding.length}`);

          chunksWithEmbeddings.push({ chunk, embedding });

          // Mise √† jour de la progression
          const progress = 50 + Math.floor((i / chunks.length) * 40);
          onProgress?.({
            stage: 'embedding',
            progress,
            message: `Embeddings: ${i + 1}/${chunks.length}`,
            currentChunk: i + 1,
            totalChunks: chunks.length,
          });

          // Log progression tous les 10 chunks
          if ((i + 1) % 10 === 0 || i === chunks.length - 1) {
            console.log(`  Embeddings: ${i + 1}/${chunks.length}`);
          }
        }

        // Batch add to all indexes (HNSW, BM25, SQLite)
        console.log(`üîç [INDEXER] Step 8: Adding ${chunksWithEmbeddings.length} chunks to indexes...`);
        console.log(`üîç [INDEXER] First embedding dimension: ${chunksWithEmbeddings[0]?.embedding?.length || 'N/A'}`);
        await (this.vectorStore as EnhancedVectorStore).addChunks(chunksWithEmbeddings);
        console.log(`üîç [INDEXER] Step 8 complete: Chunks added to HNSW, BM25, SQLite`);

        onProgress?.({
          stage: 'embedding',
          progress: 95,
          message: 'Index HNSW et BM25 construits',
        });
      } else {
        // Original behavior for VectorStore
        for (let i = 0; i < chunks.length; i++) {
          const chunk = chunks[i];

          // G√©n√©rer l'embedding
          const embedding = await this.ollamaClient.generateEmbedding(chunk.content);

          // Sauvegarder le chunk avec son embedding
          this.vectorStore.saveChunk(chunk, embedding);

          // Mise √† jour de la progression
          const progress = 50 + Math.floor((i / chunks.length) * 45);
          onProgress?.({
            stage: 'embedding',
            progress,
            message: `Embeddings: ${i + 1}/${chunks.length}`,
            currentChunk: i + 1,
            totalChunks: chunks.length,
          });

          // Log progression tous les 10 chunks
          if ((i + 1) % 10 === 0 || i === chunks.length - 1) {
            console.log(`  Embeddings: ${i + 1}/${chunks.length}`);
          }
        }
      }

      // 12. Calculer les similarit√©s avec les autres documents
      onProgress?.({
        stage: 'similarities',
        progress: 95,
        message: 'Calcul des similarit√©s avec les autres documents...',
      });

      // Get the base VectorStore for similarity calculations
      const baseStore = isEnhancedStore
        ? (this.vectorStore as EnhancedVectorStore).getBaseStore()
        : (this.vectorStore as VectorStore);

      const similaritiesCount = baseStore.computeAndSaveSimilarities(
        documentId,
        0.5 // Seuil de similarit√©
      );

      onProgress?.({
        stage: 'completed',
        progress: 100,
        message: `‚úÖ Indexation termin√©e: ${chunks.length} chunks, ${similaritiesCount} similarit√©s`,
      });

      console.log(`‚úÖ PDF index√©: ${document.title}`);
      console.log(`   - ${chunks.length} chunks`);
      console.log(`   - ${stats.totalWords} mots total`);
      console.log(`   - Moyenne: ${stats.averageWordCount} mots/chunk`);
      console.log(`   - ${similaritiesCount} similarit√©s calcul√©es`);

      return document;
    } catch (error) {
      console.error('‚ùå Erreur indexation PDF:', error);
      onProgress?.({
        stage: 'error',
        progress: 0,
        message: `Erreur: ${error}`,
      });
      throw error;
    }
  }

  /**
   * Indexe plusieurs PDFs en batch
   */
  async indexMultiplePDFs(
    filePaths: string[],
    onProgress?: (fileIndex: number, progress: IndexingProgress) => void
  ): Promise<PDFDocument[]> {
    const documents: PDFDocument[] = [];

    for (let i = 0; i < filePaths.length; i++) {
      const filePath = filePaths[i];

      console.log(`\nüìÅ Indexation ${i + 1}/${filePaths.length}: ${filePath}`);

      try {
        const document = await this.indexPDF(filePath, undefined, (progress) => {
          onProgress?.(i, progress);
        });

        documents.push(document);
      } catch (error) {
        console.error(`‚ùå Erreur avec ${filePath}:`, error);
        // Continuer avec les autres fichiers
      }
    }

    console.log(`\n‚úÖ Indexation batch termin√©e: ${documents.length}/${filePaths.length} PDFs`);

    return documents;
  }

  /**
   * R√©-indexe un document existant
   */
  async reindexPDF(documentId: string): Promise<void> {
    // R√©cup√©rer le document
    const document = this.vectorStore.getDocument(documentId);
    if (!document) {
      throw new Error(`Document introuvable: ${documentId}`);
    }

    console.log(`üîÑ R√©-indexation: ${document.title}`);

    // Supprimer l'ancien (les chunks seront supprim√©s en CASCADE)
    this.vectorStore.deleteDocument(documentId);

    // R√©-indexer
    await this.indexPDF(document.fileURL, document.bibtexKey);
  }

  /**
   * V√©rifie si Ollama est disponible
   */
  async checkOllamaAvailability(): Promise<boolean> {
    return await this.ollamaClient.isAvailable();
  }

  /**
   * Liste les mod√®les disponibles
   */
  async listAvailableModels() {
    return await this.ollamaClient.listAvailableModels();
  }

  /**
   * Obtient les statistiques de la base vectorielle
   */
  getStatistics() {
    return this.vectorStore.getStatistics();
  }

  /**
   * Nettoie les chunks orphelins
   */
  cleanOrphanedChunks() {
    return this.vectorStore.cleanOrphanedChunks();
  }

  /**
   * V√©rifie l'int√©grit√© de la base
   */
  verifyIntegrity() {
    return this.vectorStore.verifyIntegrity();
  }
}
